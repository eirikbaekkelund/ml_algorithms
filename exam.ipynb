{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_1d(x, c1, c2):\n",
    "    \"\"\" do k means algoritm until convergence\"\"\"\n",
    "    loss = 1000\n",
    "    current_loss = np.inf\n",
    "\n",
    "    epoch = 1\n",
    "    while loss < current_loss:\n",
    "        # k means clustering\n",
    "        current_loss = loss\n",
    "        # calculate distance from each point to each centroid\n",
    "        dist_c1 = np.abs(x - c1)\n",
    "        dist_c2 = np.abs(x - c2)\n",
    "\n",
    "        # assign each point to the closest centroid\n",
    "        c1_idx = dist_c1 < dist_c2\n",
    "\n",
    "        # calculate new centroids\n",
    "        c1 = np.mean(x[c1_idx])\n",
    "        c2 = np.mean(x[~c1_idx])\n",
    "\n",
    "        # calculate loss\n",
    "        loss = np.sum(dist_c1[c1_idx]) + np.sum(dist_c2[~c1_idx])\n",
    "        print(f'Epoch: {epoch}, loss: {loss}')\n",
    "\n",
    "        epoch += 1\n",
    "    return c1, c2, c1_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(x, c1, c2):\n",
    "    loss = 1000\n",
    "    current_loss = np.inf\n",
    "\n",
    "    epoch = 1\n",
    "    while loss < current_loss:\n",
    "        # k means clustering\n",
    "        current_loss = loss\n",
    "        # calculate distance from each point to each centroid\n",
    "        dist_c1 = np.linalg.norm(x - c1, axis=1)\n",
    "        dist_c2 = np.linalg.norm(x - c2, axis=1)\n",
    "\n",
    "        # assign each point to the closest centroid\n",
    "        c1_idx = dist_c1 < dist_c2\n",
    "\n",
    "        # calculate new centroids\n",
    "        c1 = np.mean(x[c1_idx], axis=0)\n",
    "        c2 = np.mean(x[~c1_idx], axis=0)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = np.sum(dist_c1[c1_idx]) + np.sum(dist_c2[~c1_idx])\n",
    "        print(f'Epoch: {epoch}, loss: {loss}')\n",
    "\n",
    "        epoch += 1\n",
    "    return c1, c2, c1_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_means(x, c1, c2, c1_idx):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plot the data and circle around centroid with max distance in centroid\n",
    "    # plt.scatter(x[:,0], x[:,1], 'o', fillstyle='none', label='data')\n",
    "    ax.scatter(c1[0], c1[1], color='r', label='centroid 1')\n",
    "    ax.scatter(c2[0], c2[1], color='b', label='centroid 2')\n",
    "\n",
    "    # calculate distance from each point to each centroid\n",
    "    max_dist_c1 = max(np.linalg.norm(x[c1_idx]- c1, axis=1))\n",
    "    max_dist_c2 = max(np.linalg.norm(x[~c1_idx]- c2, axis=1))\n",
    "\n",
    "    # plot circle around centroid with max distance in centroid\n",
    "    ax.add_patch(plt.Circle(c1, max_dist_c1, fill=False, color='r'))\n",
    "    ax.add_patch(plt.Circle(c2, max_dist_c2, fill=False, color='b'))\n",
    "\n",
    "    x1= x[c1_idx]\n",
    "    x2 = x[~c1_idx]\n",
    "\n",
    "    ax.scatter(x1[:,0], x1[:,1], color='r', label='cluster 1', marker='x')\n",
    "    ax.scatter(x2[:,0], x2[:,1], color='b', label='cluster 2', marker='x')\n",
    "\n",
    "    ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel1D(nn.Module):\n",
    "    def __init__(self, mu1, mu2, sig1, sig2):\n",
    "        super().__init__()\n",
    "        self.mu1 = mu1\n",
    "        self.mu2 = mu2\n",
    "        self.sig1 = sig1\n",
    "        self.sig2 = sig2\n",
    "        self.alpha = torch.tensor([0.5])\n",
    "\n",
    "    def gmm_gamma(self, x):\n",
    "        \"\"\" \n",
    "        Calculate the probability (responsiblity) of each point in each cluster\n",
    "        \"\"\"\n",
    "        # calculate probability of each point in each cluster\n",
    "        log_p1 = torch.distributions.Normal(loc=self.mu1, scale=self.sig1).log_prob(x)\n",
    "        log_p2 = torch.distributions.Normal(loc=self.mu2, scale=self.sig2).log_prob(x)\n",
    "\n",
    "        # calculate probability of each point in each cluster\n",
    "        p1 = torch.exp(log_p1) * self.alpha\n",
    "        p2 = torch.exp(log_p2) * (1 - self.alpha)\n",
    "        \n",
    "        gamma1 = p1 / ( p1 + p2)\n",
    "        gamma2 = p2 / ( p1 + p2)\n",
    "\n",
    "        self.alpha = torch.sum(gamma1) / x.shape[0]\n",
    "        \n",
    "        return gamma1, gamma2\n",
    "\n",
    "    def train(self, x, epochs):\n",
    "        \"\"\" \n",
    "        Calculate the parameters of a Gaussian mixture model using the EM algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            # E step\n",
    "            gamma1, gamma2 = self.gmm_gamma(x)\n",
    "            \n",
    "            # M step\n",
    "            self.mu1 = torch.sum(gamma1 * x, axis=1) / torch.sum(gamma1)\n",
    "            self.mu2 = torch.sum(gamma2 * x, axis=1) / torch.sum(gamma2)\n",
    "            \n",
    "            self.sig1 = (gamma1 * (x - self.mu1).t() @ (x - self.mu1)) / torch.sum(gamma1) * torch.eye(1)\n",
    "            self.sig2 = (gamma2 * (x - self.mu2).t() @ (x - self.mu2)) / torch.sum(gamma2) * torch.eye(1)\n",
    "\n",
    "        return self.mu1, self.mu2, self.sig1, self.sig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetOperations(nn.Module):\n",
    "    \"\"\" \n",
    "    Class that performs a series of convolutional operations\n",
    "\n",
    "    Args:\n",
    "        conv_ops (list): list of tuples (filters, stride, kernel_size, padding)\n",
    "        channels (int): number of channels in the input\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_ops, channels=3, bias=True):\n",
    "        super(ConvNetOperations, self).__init__()\n",
    "        self.conv_ops = conv_ops\n",
    "        self.bias = bias\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.channels = channels\n",
    "        self.layer_outputs = []\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "     \n",
    "        for filters, stride, kernel_size, padding in self.conv_ops:\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels=self.channels, \n",
    "                                              out_channels=filters, \n",
    "                                              kernel_size=kernel_size, \n",
    "                                              stride=stride, \n",
    "                                              padding=padding,\n",
    "                                              bias=self.bias))\n",
    "            self.channels = filters\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "            self.layer_outputs.append(x.shape[1:])\n",
    "        return x\n",
    "    \n",
    "    def get_output_size(self, input_size):\n",
    "        x = torch.randn(1, *input_size)\n",
    "        return self.forward(x).shape[1:]\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum([np.prod(p.shape) for p in self.parameters()])\n",
    "    \n",
    "    def get_num_params_per_layer(self):\n",
    "        params = [np.prod(p.shape) for p in self.parameters()]\n",
    "        return [sum(params[i:i+2]) for i in range(0, len(params), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture Models\n",
    "class GaussianMixtureModel2D(nn.Module):\n",
    "    def __init__(self, mu1, mu2, sig1, sig2):\n",
    "        super().__init__()\n",
    "        self.mu1 = mu1\n",
    "        self.mu2 = mu2\n",
    "        self.sig1 = sig1\n",
    "        self.sig2 = sig2\n",
    "        self.alpha = torch.tensor([0.5])\n",
    "\n",
    "    def gmm_gamma(self, x):\n",
    "        \"\"\" \n",
    "        Calculate the probability (responsiblity) of each point in each cluster\n",
    "        \"\"\"\n",
    "        # calculate probability of each point in each cluster\n",
    "        log_p1 = torch.distributions.multivariate_normal.MultivariateNormal(self.mu1, self.sig1).log_prob(x) \n",
    "        log_p2 = torch.distributions.multivariate_normal.MultivariateNormal(self.mu2, self.sig2).log_prob(x) \n",
    "\n",
    "        # calculate probability of each point in each cluster\n",
    "        p1 = torch.exp(log_p1) * self.alpha\n",
    "        p2 = torch.exp(log_p2) * (1 - self.alpha)\n",
    "        \n",
    "        gamma1 = p1 / ( p1 + p2)\n",
    "        gamma2 = p2 / ( p1 + p2)\n",
    "\n",
    "        self.alpha = torch.sum(gamma1) / x.shape[0]\n",
    "        \n",
    "        return gamma1, gamma2\n",
    "\n",
    "    def train(self, x, epochs):\n",
    "        \"\"\" \n",
    "        Calculate the parameters of a Gaussian mixture model using the EM algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            # E step\n",
    "            gamma1, gamma2 = self.gmm_gamma(x)\n",
    "            \n",
    "            # M step\n",
    "            self.mu1 = torch.sum(gamma1 * x.T, axis=1) / torch.sum(gamma1)\n",
    "            self.mu2 = torch.sum(gamma2 * x.T, axis=1) / torch.sum(gamma2)\n",
    "            \n",
    "            self.sig1 = (gamma1 * (x - self.mu1).t() @ (x - self.mu1)) / torch.sum(gamma1)\n",
    "            self.sig2 = (gamma2 * (x - self.mu2).t() @ (x - self.mu2)) / torch.sum(gamma2)\n",
    "\n",
    "        return self.mu1, self.mu2, self.sig1, self.sig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution contours of the GMM \n",
    "def plot_gmm2d(x, gmm, c1_dist, c2_dist, c1_idx, mu1, mu2):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "    xx1, xx2 = np.meshgrid(x1, x2)\n",
    "    zz = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "    p1 = torch.exp(c1_dist.log_prob(torch.tensor(zz, dtype=torch.float32))) * gmm.alpha\n",
    "    p2 = torch.exp(c2_dist.log_prob(torch.tensor(zz, dtype=torch.float32))) * (1 - gmm.alpha)\n",
    "\n",
    "    # plot contours\n",
    "    ax.contourf(x1, x2, p1.reshape(100,100), cmap='Reds', alpha=0.3)\n",
    "    ax.contourf(x1, x2, p2.reshape(100,100), cmap='Blues', alpha=0.3)\n",
    "\n",
    "    # plot data\n",
    "    ax.scatter(x[:,0], x[:,1], color='k', marker='x')\n",
    "\n",
    "    # plot cluster 1\n",
    "    ax.scatter(x[c1_idx][:,0], x[c1_idx][:,1], color='r', marker='x')\n",
    "\n",
    "    # plot cluster 2\n",
    "    ax.scatter(x[~c1_idx][:,0], x[~c1_idx][:,1], color='b', marker='x')\n",
    "\n",
    "    # plot centroid of each cluster\n",
    "    ax.scatter(mu1[0], mu1[1], color='r', marker='o')\n",
    "    ax.scatter(mu2[0], mu2[1], color='b', marker='o')\n",
    "\n",
    "    ax.set_xlim(0-0.01,1+0.01)\n",
    "    ax.set_ylim(0-0.01,1+0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_proportions(class_counts):\n",
    "    \"\"\" \n",
    "    Calculates the proportion of samples in each node \n",
    "    relative to the total number of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    node1 = np.sum(class_counts['node1'])\n",
    "    node2 = np.sum(class_counts['node2'])\n",
    "    n = node1 + node2\n",
    "    return node1 / n, node2 / n\n",
    "\n",
    "def misclassification_error(class_counts, y):\n",
    "    \"\"\" \n",
    "    Calculate misclassification error for the split.\n",
    "    \"\"\"\n",
    "    node1 = class_counts['node1']\n",
    "    node2 = class_counts['node2']\n",
    "    \n",
    "    # calculate misclassification error per node\n",
    "    error1 = 1 - np.max(node1) / np.sum(node1)\n",
    "    error2 = 1 - np.max(node2) / np.sum(node2)\n",
    "\n",
    "    error =  (error1 +  error2) / 2\n",
    "\n",
    "    return error\n",
    "\n",
    "def gini_impurity_error(class_counts, y):\n",
    "    \"\"\" \n",
    "    Calculate gini impurity error for the split.\n",
    "    \"\"\"\n",
    "    node1 = class_counts['node1']\n",
    "    node2 = class_counts['node2']\n",
    "    \n",
    "    # calculate gini impurity per node\n",
    "    gini1 = 1 - np.sum((node1 / np.sum(node1)) ** 2)\n",
    "    gini2 = 1 - np.sum((node2 / np.sum(node2)) ** 2)\n",
    "\n",
    "    prop1, prop2 = calculate_node_proportions(class_counts)\n",
    "    \n",
    "    # calculate weighted gini impurity\n",
    "    gini =  prop1 * gini1 + prop2 * gini2\n",
    "\n",
    "    return gini\n",
    "\n",
    "def cross_entropy_error(class_counts, y):\n",
    "    \"\"\" \n",
    "    Calculate cross entropy error for the split.\n",
    "    \"\"\"\n",
    "    node1 = class_counts['node1']\n",
    "    node2 = class_counts['node2']\n",
    "    \n",
    "    node1 = node1[node1 != 0]\n",
    "    node2 = node2[node2 != 0]\n",
    "    \n",
    "    # calculate cross entropy for present classes in each node\n",
    "    cross_entropy1 = - np.sum(node1 / np.sum(node1) * np.log(node1 / np.sum(node1)))\n",
    "    cross_entropy2 = - np.sum(node2 / np.sum(node2) * np.log(node2 / np.sum(node2)))\n",
    "\n",
    "    prop1, prop2 = calculate_node_proportions(class_counts)\n",
    "    \n",
    "    # calculate weighted cross entropy\n",
    "    cross_entropy =  prop1 * cross_entropy1 + prop2 * cross_entropy2\n",
    "\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
